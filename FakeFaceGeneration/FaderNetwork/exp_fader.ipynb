{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From README.md"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FaderNetworks\n",
    "\n",
    "PyTorch implementation of [Fader Networks](https://arxiv.org/pdf/1706.00409.pdf) (NIPS 2017).\n",
    "\n",
    "<p align=\"center\"><a href=https://github.com/facebookresearch/FaderNetworks/blob/master/images/interpolation.jpg?raw=true><img width=\"100%\" src=\"./images/interpolation.jpg\" /></a></p>\n",
    "\n",
    "Fader Networks can generate different realistic versions of images by modifying attributes such as gender or age group. They can swap multiple attributes at a time, and continuously interpolate between each attribute value. In this repository we provide the code to reproduce the results presented in the paper, as well as trained models.\n",
    "\n",
    "### Single-attribute swap\n",
    "\n",
    "Below are some examples of different attribute swaps:\n",
    "\n",
    "<p align=\"center\"><a href=https://github.com/facebookresearch/FaderNetworks/blob/master/images/swap.jpg?raw=true><img width=\"100%\" src=\"./images/swap.jpg\" /></a></p>\n",
    "\n",
    "### Multi-attributes swap\n",
    "\n",
    "The Fader Networks are also designed to disentangle multiple attributes at a time:\n",
    "\n",
    "<p align=\"center\"><a href=https://github.com/facebookresearch/FaderNetworks/blob/master/images/multi_attr.jpg?raw=true><img width=\"100%\" src=\"./images/multi_attr.jpg\" /></a></p>\n",
    "\n",
    "## Model\n",
    "\n",
    "<p align=\"center\"><a href=https://github.com/facebookresearch/FaderNetworks/blob/master/images/v3.png?raw=true><img width=\"70%\" src=\"./images/v3.png\" /></a></p>\n",
    "\n",
    "The main branch of the model (Inference Model), is an autoencoder of images. Given an image `x` and an attribute `y` (e.g. male/female), the decoder is trained to reconstruct the image from the latent state `E(x)` and `y`. The other branch (Adversarial Component), is composed of a discriminator trained to predict the attribute from the latent state. The encoder of the Inference Model is trained not only to reconstruct the image, but also to fool the discriminator, by removing from `E(x)` the information related to the attribute. As a result, the decoder needs to consider `y` to properly reconstruct the image. During training, the model is trained using real attribute values, but at test time, `y` can be manipulated to generate variations of the original image.\n",
    "\n",
    "## Dependencies\n",
    "* Python 2/3 with [NumPy](http://www.numpy.org/)/[SciPy](https://www.scipy.org/)\n",
    "* [PyTorch](http://pytorch.org/)\n",
    "* OpenCV\n",
    "* CUDA\n",
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "Simply clone the repository:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/facebookresearch/FaderNetworks.git\n",
    "cd FaderNetworks\n",
    "```\n",
    "\n",
    "## Dataset\n",
    "Download the aligned and cropped CelebA dataset from http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html. Extract all images and move them to the `data/img_align_celeba/` folder. There should be 202599 images. The dataset also provides a file `list_attr_celeba.txt` containing the list of the 40 attributes associated with each image. Move it to `data/`. Then simply run:\n",
    "\n",
    "```batch\n",
    "cd data\n",
    "./preprocess.py # python ./preprocess.py\n",
    "```\n",
    "\n",
    "It will resize images, and create 2 files: `images_256_256.pth` and `attributes.pth`. The first one contains a tensor of size `(202599, 3, 256, 256)` containing the concatenation of all resized images. Note that you can update the image size in `preprocess.py` to work with different resolutions. The second file is a pre-processed version of the attributes.\n",
    "\n",
    "## Pretrained models\n",
    "You can download pretrained classifiers and Fader Networks by running:\n",
    "\n",
    "```batch\n",
    "cd models\n",
    "./download.sh\n",
    "```\n",
    "\n",
    "## Train your own models\n",
    "\n",
    "### Train a classifier\n",
    "To train your own model you first need to train a classifier to let the model evaluate the swap quality during the training. Training a good classifier is relatively simple for most attributes, and a good model can be trained in a few minutes. We provide a trained classifier for all attributes in `models/classifier256.pth`. Note that the classifier does not need to be state-of-the-art, it is not used during the training process, but is just here to monitor the swap quality. If you want to train your own classifier, you can run `classifier.py`, using the following parameters:\n",
    "\n",
    "\n",
    "```bash\n",
    "python classifier.py\n",
    "\n",
    "# Main parameters\n",
    "--img_sz 256                  # image size\n",
    "--img_fm 3                    # number of feature maps\n",
    "--attr \"*\"                    # attributes list. \"*\" for all attributes\n",
    "\n",
    "# Network architecture\n",
    "--init_fm 32                  # number of feature maps in the first layer\n",
    "--max_fm 512                  # maximum number of feature maps\n",
    "--hid_dim 512                 # hidden layer size\n",
    "\n",
    "# Training parameters\n",
    "--v_flip False                # randomly flip images vertically (data augmentation)\n",
    "--h_flip True                 # randomly flip images horizontally (data augmentation)\n",
    "--batch_size 32               # batch size\n",
    "--optimizer \"adam,lr=0.0002\"  # optimizer\n",
    "--clip_grad_norm 5            # clip gradient L2 norm\n",
    "--n_epochs 1000               # number of epochs\n",
    "--epoch_size 50000            # number of images per epoch\n",
    "\n",
    "# Reload\n",
    "--reload \"\"                   # reload a trained classifier\n",
    "--debug False                 # debug mode (if True, load a small subset of the dataset)\n",
    "```\n",
    "\n",
    "\n",
    "### Train a Fader Network\n",
    "\n",
    "You can train a Fader Network with `train.py`. The autoencoder can receive feedback from:\n",
    "- The image reconstruction loss\n",
    "- The latent discriminator loss\n",
    "- The PatchGAN discriminator loss\n",
    "- The classifier loss\n",
    "\n",
    "In the paper, only the first two losses are used, but the two others could improve the results further. You can tune the impact of each of these losses with the lambda_ae, lambda_lat_dis, lambda_ptc_dis, and lambda_clf_dis coefficients. Below is a complete list of all parameters:\n",
    "\n",
    "```bash\n",
    "# Main parameters\n",
    "--img_sz 256                      # image size\n",
    "--img_fm 3                        # number of feature maps\n",
    "--attr \"Male\"                     # attributes list. \"*\" for all attributes\n",
    "\n",
    "# Networks architecture\n",
    "--instance_norm False             # use instance normalization instead of batch normalization\n",
    "--init_fm 32                      # number of feature maps in the first layer\n",
    "--max_fm 512                      # maximum number of feature maps\n",
    "--n_layers 6                      # number of layers in the encoder / decoder\n",
    "--n_skip 0                        # number of skip connections\n",
    "--deconv_method \"convtranspose\"   # deconvolution method\n",
    "--hid_dim 512                     # hidden layer size\n",
    "--dec_dropout 0                   # dropout in the decoder\n",
    "--lat_dis_dropout 0.3             # dropout in the latent discriminator\n",
    "\n",
    "# Training parameters\n",
    "--n_lat_dis 1                     # number of latent discriminator training steps\n",
    "--n_ptc_dis 0                     # number of PatchGAN discriminator training steps\n",
    "--n_clf_dis 0                     # number of classifier training steps\n",
    "--smooth_label 0.2                # smooth discriminator labels\n",
    "--lambda_ae 1                     # autoencoder loss coefficient\n",
    "--lambda_lat_dis 0.0001           # latent discriminator loss coefficient\n",
    "--lambda_ptc_dis 0                # PatchGAN discriminator loss coefficient\n",
    "--lambda_clf_dis 0                # classifier loss coefficient\n",
    "--lambda_schedule 500000          # lambda scheduling (0 to disable)\n",
    "--v_flip False                    # randomly flip images vertically (data augmentation)\n",
    "--h_flip True                     # randomly flip images horizontally (data augmentation)\n",
    "--batch_size 32                   # batch size\n",
    "--ae_optimizer \"adam,lr=0.0002\"   # autoencoder optimizer\n",
    "--dis_optimizer \"adam,lr=0.0002\"  # discriminator optimizer\n",
    "--clip_grad_norm 5                # clip gradient L2 norm\n",
    "--n_epochs 1000                   # number of epochs\n",
    "--epoch_size 50000                # number of images per epoch\n",
    "\n",
    "# Reload\n",
    "--ae_reload \"\"                    # reload pretrained autoencoder\n",
    "--lat_dis_reload \"\"               # reload pretrained latent discriminator\n",
    "--ptc_dis_reload \"\"               # reload pretrained PatchGAN discriminator\n",
    "--clf_dis_reload \"\"               # reload pretrained classifier\n",
    "--eval_clf \"\"                     # evaluation classifier (trained with classifier.py)\n",
    "--debug False                     # debug mode (if True, load a small subset of the dataset)\n",
    "```\n",
    "\n",
    "## Generate interpolations\n",
    "\n",
    "Given a trained model, you can use it to swap attributes of images in the dataset. Below are examples using the pretrained models:\n",
    "\n",
    "```bash\n",
    "# Narrow Eyes\n",
    "python interpolate.py --model_path models/narrow_eyes.pth --n_images 10 --n_interpolations 10 --alpha_min 10.0 --alpha_max 10.0 --output_path narrow_eyes.png\n",
    "\n",
    "# Eyeglasses\n",
    "python interpolate.py --model_path models/eyeglasses.pth --n_images 10 --n_interpolations 10 --alpha_min 2.0 --alpha_max 2.0 --output_path eyeglasses.png\n",
    "\n",
    "# Age\n",
    "python interpolate.py --model_path models/young.pth --n_images 10 --n_interpolations 10 --alpha_min 10.0 --alpha_max 10.0 --output_path young.png\n",
    "\n",
    "# Gender\n",
    "python interpolate.py --model_path models/male.pth --n_images 10 --n_interpolations 10 --alpha_min 2.0 --alpha_max 2.0 --output_path male.png\n",
    "\n",
    "# Pointy nose\n",
    "python interpolate.py --model_path models/pointy_nose.pth --n_images 10 --n_interpolations 10 --alpha_min 10.0 --alpha_max 10.0 --output_path pointy_nose.png\n",
    "```\n",
    "\n",
    "These commands will generate images with 10 rows of 12 columns with the interpolated images. The first column corresponds to the original image, the second is the reconstructed image (without alteration of the attribute), and the remaining ones correspond to the interpolated images. `alpha_min` and `alpha_max` represent the range of the interpolation. Values superior to 1 represent generations over the True / False range of the boolean attribute in the model. Note that the variations of some attributes may only be noticeable for high values of alphas. For instance, for the \"eyeglasses\" or \"gender\" attributes, alpha_max=2 is usually enough, while for the \"age\" or \"narrow eyes\" attributes, it is better to go up to alpha_max=10.\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "If you find this code useful, please consider citing:\n",
    "\n",
    "[*Fader Networks: Manipulating Images by Sliding Attributes*](https://arxiv.org/pdf/1706.00409.pdf) - G. Lample, N. Zeghidour, N. Usunier, A. Bordes, L. Denoyer, M'A. Ranzato\n",
    "\n",
    "```\n",
    "@inproceedings{lample2017fader,\n",
    "  title={Fader Networks: Manipulating Images by Sliding Attributes},\n",
    "  author={Lample, Guillaume and Zeghidour, Neil and Usunier, Nicolas and Bordes, Antoine and DENOYER, Ludovic and others},\n",
    "  booktitle={Advances in Neural Information Processing Systems},\n",
    "  pages={5963--5972},\n",
    "  year={2017}\n",
    "}\n",
    "```\n",
    "\n",
    "Contact: [gl@fb.com](mailto:gl@fb.com), [neilz@fb.com](mailto:neilz@fb.com)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ranjani\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=r'C:\\GitHub\\Smart-Education-data\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    #Celeba\n",
    "    raw_img_path=os.path.join(data_path, 'celeba')\n",
    "    out_img_path=os.path.join(data_path, 'celeba-out')\n",
    "    list_attr_path=os.path.join(data_path, 'list_attr_celeba.txt')\n",
    "if False:\n",
    "    #Celeba-HQ\n",
    "    raw_img_path=os.path.join(data_path, 'celeba-hq')\n",
    "    out_img_path=os.path.join(data_path, 'celeba-hq-out')\n",
    "    # list_attr_path=?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202599\n",
      "C:\\GitHub\\Smart-Education-data\\data\\celeba-out\\images_128_128.pth\n"
     ]
    }
   ],
   "source": [
    "# N_IMAGES = 202599#5#200 # => let's use glob\n",
    "N_IMAGES = len(glob.glob(os.path.join(raw_img_path, '*.*')))\n",
    "print(N_IMAGES)\n",
    "# IMG_SIZE = 256\n",
    "IMG_SIZE = 128 #same as attgan\n",
    "IMG_PATH = os.path.join(out_img_path, 'images_%i_%i.pth' % (IMG_SIZE, IMG_SIZE)) #path for image binaries to be saved\n",
    "ATTR_PATH = r'C:/GitHub/Smart-Education-data/data/attributes.pth' #attributes will be saved as pth file\n",
    "\n",
    "print(IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(raw_img_path, N_IMAGES, IMG_SIZE, IMG_PATH):\n",
    "    if os.path.isfile(IMG_PATH):\n",
    "        print(\"%s exists, nothing to do.\" % IMG_PATH)\n",
    "        return \n",
    "\n",
    "    # print(\"Reading images from img_align_celeba/ ...\")\n",
    "    # raw_images = []\n",
    "    # for i in range(1, N_IMAGES + 1):\n",
    "    #     if i % 10000 == 0:\n",
    "    #     # if i % 10 == 0:\n",
    "    #         print(i)\n",
    "    #     raw_images.append(mpimg.imread(os.path.join(raw_img_path, '%06i.jpg' % i))[20:-20])\n",
    "    \n",
    "    # print('debugging1')\n",
    "    # print(len(raw_images))\n",
    "\n",
    "    # if len(raw_images) != N_IMAGES:\n",
    "        # raise Exception(\"Found %i images. Expected %i\" % (len(raw_images), N_IMAGES))\n",
    "    print(\"Resizing images ...\")\n",
    "    all_images = []\n",
    "    # for i, image in enumerate(raw_images):\n",
    "    for i in range(1, N_IMAGES + 1): #####\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        image=mpimg.imread(os.path.join(raw_img_path, '%06i.jpg' % i))[20:-20] #####\n",
    "        # assert image.shape == (178, 178, 3)\n",
    "        if IMG_SIZE < 178:\n",
    "            image = cv2.resize(image, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
    "        elif IMG_SIZE > 178:\n",
    "            image = cv2.resize(image, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_LANCZOS4)\n",
    "        # print(image.shape)\n",
    "        assert image.shape == (IMG_SIZE, IMG_SIZE, 3)\n",
    "        all_images.append(image)\n",
    "\n",
    "    data = np.concatenate([img.transpose((2, 0, 1))[None] for img in all_images], 0)\n",
    "    data = torch.from_numpy(data)\n",
    "    assert data.size() == (N_IMAGES, 3, IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "    print(\"Saving images to %s ...\" % IMG_PATH)\n",
    "    torch.save(data[:20000].clone(), os.path.join(out_img_path, 'images_%i_%i_20000.pth' % (IMG_SIZE, IMG_SIZE)))\n",
    "    torch.save(data, IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_attributes(list_attr_path, N_IMAGES, ATTR_PATH):\n",
    "\n",
    "    if os.path.isfile(ATTR_PATH):\n",
    "        print(\"%s exists, nothing to do.\" % ATTR_PATH)\n",
    "        return\n",
    "\n",
    "    attr_lines = [line.rstrip() for line in open(list_attr_path, 'r')]\n",
    "    assert len(attr_lines) == N_IMAGES + 2\n",
    "\n",
    "    print('debug:: ', attr_lines)\n",
    "    \n",
    "    attr_keys = attr_lines[1].split()\n",
    "    attributes = {k: np.zeros(N_IMAGES, dtype=np.bool) for k in attr_keys}\n",
    "\n",
    "    for i, line in enumerate(attr_lines[2:]):\n",
    "        image_id = i + 1\n",
    "        split = line.split()\n",
    "        assert len(split) == 41\n",
    "        assert split[0] == ('%06i.jpg' % image_id)\n",
    "        # assert split[0] == ('%03i.jpg' % image_id)\n",
    "        assert all(x in ['-1', '1'] for x in split[1:])\n",
    "        for j, value in enumerate(split[1:]):\n",
    "            attributes[attr_keys[j]][i] = value == '1'\n",
    "\n",
    "    print(\"Saving attributes to %s ...\" % ATTR_PATH)\n",
    "    torch.save(attributes, ATTR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_images(raw_img_path, N_IMAGES, IMG_SIZE, IMG_PATH)\n",
    "# preprocess_attributes(list_attr_path, N_IMAGES, ATTR_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def build_layers(img_sz, img_fm, init_fm, max_fm, n_layers, n_attr, n_skip,\n",
    "                 deconv_method, instance_norm, enc_dropout, dec_dropout):\n",
    "    \"\"\"\n",
    "    Build auto-encoder layers.\n",
    "    \"\"\"\n",
    "    assert init_fm <= max_fm\n",
    "    assert n_skip <= n_layers - 1\n",
    "    assert np.log2(img_sz).is_integer()\n",
    "    assert n_layers <= int(np.log2(img_sz))\n",
    "    assert type(instance_norm) is bool\n",
    "    assert 0 <= enc_dropout < 1\n",
    "    assert 0 <= dec_dropout < 1\n",
    "    norm_fn = nn.InstanceNorm2d if instance_norm else nn.BatchNorm2d\n",
    "\n",
    "    enc_layers = []\n",
    "    dec_layers = []\n",
    "\n",
    "    n_in = img_fm\n",
    "    n_out = init_fm\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        enc_layer = []\n",
    "        dec_layer = []\n",
    "        skip_connection = n_layers - (n_skip + 1) <= i < n_layers - 1\n",
    "        n_dec_in = n_out + n_attr + (n_out if skip_connection else 0)\n",
    "        n_dec_out = n_in\n",
    "\n",
    "        # encoder layer\n",
    "        enc_layer.append(nn.Conv2d(n_in, n_out, 4, 2, 1))\n",
    "        if i > 0:\n",
    "            enc_layer.append(norm_fn(n_out, affine=True))\n",
    "        enc_layer.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        if enc_dropout > 0:\n",
    "            enc_layer.append(nn.Dropout(enc_dropout))\n",
    "\n",
    "        # decoder layer\n",
    "        if deconv_method == 'upsampling':\n",
    "            dec_layer.append(nn.UpsamplingNearest2d(scale_factor=2))\n",
    "            dec_layer.append(nn.Conv2d(n_dec_in, n_dec_out, 3, 1, 1))\n",
    "        elif deconv_method == 'convtranspose':\n",
    "            dec_layer.append(nn.ConvTranspose2d(n_dec_in, n_dec_out, 4, 2, 1, bias=False))\n",
    "        else:\n",
    "            assert deconv_method == 'pixelshuffle'\n",
    "            dec_layer.append(nn.Conv2d(n_dec_in, n_dec_out * 4, 3, 1, 1))\n",
    "            dec_layer.append(nn.PixelShuffle(2))\n",
    "        if i > 0:\n",
    "            dec_layer.append(norm_fn(n_dec_out, affine=True))\n",
    "            if dec_dropout > 0 and i >= n_layers - 3:\n",
    "                dec_layer.append(nn.Dropout(dec_dropout))\n",
    "            dec_layer.append(nn.ReLU(inplace=True))\n",
    "        else:\n",
    "            dec_layer.append(nn.Tanh())\n",
    "\n",
    "        # update\n",
    "        n_in = n_out\n",
    "        n_out = min(2 * n_out, max_fm)\n",
    "        enc_layers.append(nn.Sequential(*enc_layer))\n",
    "        dec_layers.insert(0, nn.Sequential(*dec_layer))\n",
    "\n",
    "    return enc_layers, dec_layers\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.img_sz = params.img_sz\n",
    "        self.img_fm = params.img_fm\n",
    "        self.instance_norm = params.instance_norm\n",
    "        self.init_fm = params.init_fm\n",
    "        self.max_fm = params.max_fm\n",
    "        self.n_layers = params.n_layers\n",
    "        self.n_skip = params.n_skip\n",
    "        self.deconv_method = params.deconv_method\n",
    "        self.dropout = params.dec_dropout\n",
    "        self.attr = params.attr\n",
    "        self.n_attr = params.n_attr\n",
    "\n",
    "        enc_layers, dec_layers = build_layers(self.img_sz, self.img_fm, self.init_fm,\n",
    "                                              self.max_fm, self.n_layers, self.n_attr,\n",
    "                                              self.n_skip, self.deconv_method,\n",
    "                                              self.instance_norm, 0, self.dropout)\n",
    "        self.enc_layers = nn.ModuleList(enc_layers)\n",
    "        self.dec_layers = nn.ModuleList(dec_layers)\n",
    "\n",
    "    def encode(self, x):\n",
    "        assert x.size()[1:] == (self.img_fm, self.img_sz, self.img_sz)\n",
    "\n",
    "        enc_outputs = [x]\n",
    "        for layer in self.enc_layers:\n",
    "            enc_outputs.append(layer(enc_outputs[-1]))\n",
    "\n",
    "        assert len(enc_outputs) == self.n_layers + 1\n",
    "        return enc_outputs\n",
    "\n",
    "    def decode(self, enc_outputs, y):\n",
    "        bs = enc_outputs[0].size(0)\n",
    "        assert len(enc_outputs) == self.n_layers + 1\n",
    "        assert y.size() == (bs, self.n_attr)\n",
    "\n",
    "        dec_outputs = [enc_outputs[-1]]\n",
    "        y = y.unsqueeze(2).unsqueeze(3)\n",
    "        for i, layer in enumerate(self.dec_layers):\n",
    "            size = dec_outputs[-1].size(2)\n",
    "            # attributes\n",
    "            input = [dec_outputs[-1], y.expand(bs, self.n_attr, size, size)]\n",
    "            # skip connection\n",
    "            if 0 < i <= self.n_skip:\n",
    "                input.append(enc_outputs[-1 - i])\n",
    "            input = torch.cat(input, 1)\n",
    "            dec_outputs.append(layer(input))\n",
    "\n",
    "        assert len(dec_outputs) == self.n_layers + 1\n",
    "        assert dec_outputs[-1].size() == (bs, self.img_fm, self.img_sz, self.img_sz)\n",
    "        return dec_outputs\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        enc_outputs = self.encode(x)\n",
    "        dec_outputs = self.decode(enc_outputs, y)\n",
    "        return enc_outputs, dec_outputs\n",
    "\n",
    "\n",
    "class LatentDiscriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(LatentDiscriminator, self).__init__()\n",
    "\n",
    "        self.img_sz = params.img_sz\n",
    "        self.img_fm = params.img_fm\n",
    "        self.init_fm = params.init_fm\n",
    "        self.max_fm = params.max_fm\n",
    "        self.n_layers = params.n_layers\n",
    "        self.n_skip = params.n_skip\n",
    "        self.hid_dim = params.hid_dim\n",
    "        self.dropout = params.lat_dis_dropout\n",
    "        self.attr = params.attr\n",
    "        self.n_attr = params.n_attr\n",
    "\n",
    "        self.n_dis_layers = int(np.log2(self.img_sz))\n",
    "        self.conv_in_sz = self.img_sz / (2 ** (self.n_layers - self.n_skip))\n",
    "        self.conv_in_fm = min(self.init_fm * (2 ** (self.n_layers - self.n_skip - 1)), self.max_fm)\n",
    "        self.conv_out_fm = min(self.init_fm * (2 ** (self.n_dis_layers - 1)), self.max_fm)\n",
    "\n",
    "        # discriminator layers are identical to encoder, but convolve until size 1\n",
    "        enc_layers, _ = build_layers(self.img_sz, self.img_fm, self.init_fm, self.max_fm,\n",
    "                                     self.n_dis_layers, self.n_attr, 0, 'convtranspose',\n",
    "                                     False, self.dropout, 0)\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*(enc_layers[self.n_layers - self.n_skip:]))\n",
    "        self.proj_layers = nn.Sequential(\n",
    "            nn.Linear(self.conv_out_fm, self.hid_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(self.hid_dim, self.n_attr)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.size()[1:] == (self.conv_in_fm, self.conv_in_sz, self.conv_in_sz)\n",
    "        conv_output = self.conv_layers(x)\n",
    "        assert conv_output.size() == (x.size(0), self.conv_out_fm, 1, 1)\n",
    "        return self.proj_layers(conv_output.view(x.size(0), self.conv_out_fm))\n",
    "\n",
    "\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "\n",
    "        self.img_sz = params.img_sz\n",
    "        self.img_fm = params.img_fm\n",
    "        self.init_fm = params.init_fm\n",
    "        self.max_fm = params.max_fm\n",
    "        self.n_patch_dis_layers = 3\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(self.img_fm, self.init_fm, kernel_size=4, stride=2, padding=1))\n",
    "        layers.append(nn.LeakyReLU(0.2, True))\n",
    "\n",
    "        n_in = self.init_fm\n",
    "        n_out = min(2 * n_in, self.max_fm)\n",
    "\n",
    "        for n in range(self.n_patch_dis_layers):\n",
    "            stride = 1 if n == self.n_patch_dis_layers - 1 else 2\n",
    "            layers.append(nn.Conv2d(n_in, n_out, kernel_size=4, stride=stride, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(n_out))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            if n < self.n_patch_dis_layers - 1:\n",
    "                n_in = n_out\n",
    "                n_out = min(2 * n_out, self.max_fm)\n",
    "\n",
    "        layers.append(nn.Conv2d(n_out, 1, kernel_size=4, stride=1, padding=1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.dim() == 4\n",
    "        return self.layers(x).view(x.size(0), -1).mean(1).view(x.size(0))\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.img_sz = params.img_sz\n",
    "        self.img_fm = params.img_fm\n",
    "        self.init_fm = params.init_fm\n",
    "        self.max_fm = params.max_fm\n",
    "        self.hid_dim = params.hid_dim\n",
    "        self.attr = params.attr\n",
    "        self.n_attr = params.n_attr\n",
    "\n",
    "        self.n_clf_layers = int(np.log2(self.img_sz))\n",
    "        self.conv_out_fm = min(self.init_fm * (2 ** (self.n_clf_layers - 1)), self.max_fm)\n",
    "\n",
    "        # classifier layers are identical to encoder, but convolve until size 1\n",
    "        enc_layers, _ = build_layers(self.img_sz, self.img_fm, self.init_fm, self.max_fm,\n",
    "                                     self.n_clf_layers, self.n_attr, 0, 'convtranspose',\n",
    "                                     False, 0, 0)\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*enc_layers)\n",
    "        self.proj_layers = nn.Sequential(\n",
    "            nn.Linear(self.conv_out_fm, self.hid_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(self.hid_dim, self.n_attr)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.size()[1:] == (self.img_fm, self.img_sz, self.img_sz)\n",
    "        conv_output = self.conv_layers(x)\n",
    "        assert conv_output.size() == (x.size(0), self.conv_out_fm, 1, 1)\n",
    "        return self.proj_layers(conv_output.view(x.size(0), self.conv_out_fm))\n",
    "\n",
    "\n",
    "def get_attr_loss(output, attributes, flip, params):\n",
    "    \"\"\"\n",
    "    Compute attributes loss.\n",
    "    \"\"\"\n",
    "    assert type(flip) is bool\n",
    "    k = 0\n",
    "    loss = 0\n",
    "    for (_, n_cat) in params.attr:\n",
    "        # categorical\n",
    "        x = output[:, k:k + n_cat].contiguous()\n",
    "        y = attributes[:, k:k + n_cat].max(1)[1].view(-1)\n",
    "        if flip:\n",
    "            # generate different categories\n",
    "            shift = torch.LongTensor(y.size()).random_(n_cat - 1) + 1\n",
    "            y = (y + Variable(shift.cuda())) % n_cat\n",
    "        loss += F.cross_entropy(x, y)\n",
    "        k += n_cat\n",
    "    return loss\n",
    "\n",
    "\n",
    "def update_predictions(all_preds, preds, targets, params):\n",
    "    \"\"\"\n",
    "    Update discriminator / classifier predictions.\n",
    "    \"\"\"\n",
    "    assert len(all_preds) == len(params.attr)\n",
    "    k = 0\n",
    "    for j, (_, n_cat) in enumerate(params.attr):\n",
    "        _preds = preds[:, k:k + n_cat].max(1)[1]\n",
    "        _targets = targets[:, k:k + n_cat].max(1)[1]\n",
    "        all_preds[j].extend((_preds == _targets).tolist())\n",
    "        k += n_cat\n",
    "    assert k == params.n_attr\n",
    "\n",
    "\n",
    "def get_mappings(params):\n",
    "    \"\"\"\n",
    "    Create a mapping between attributes and their associated IDs.\n",
    "    \"\"\"\n",
    "    if not hasattr(params, 'mappings'):\n",
    "        mappings = []\n",
    "        k = 0\n",
    "        for (_, n_cat) in params.attr:\n",
    "            assert n_cat >= 2\n",
    "            mappings.append((k, k + n_cat))\n",
    "            k += n_cat\n",
    "        assert k == params.n_attr\n",
    "        params.mappings = mappings\n",
    "    return params.mappings\n",
    "\n",
    "\n",
    "def flip_attributes(attributes, params, attribute_id, new_value=None):\n",
    "    \"\"\"\n",
    "    Randomly flip a set of attributes.\n",
    "    \"\"\"\n",
    "    assert attributes.size(1) == params.n_attr\n",
    "    mappings = get_mappings(params)\n",
    "    attributes = attributes.data.clone().cpu()\n",
    "\n",
    "    def flip_attribute(attribute_id, new_value=None):\n",
    "        bs = attributes.size(0)\n",
    "        i, j = mappings[attribute_id]\n",
    "        attributes[:, i:j].zero_()\n",
    "        if new_value is None:\n",
    "            y = torch.LongTensor(bs).random_(j - i)\n",
    "        else:\n",
    "            assert new_value in range(j - i)\n",
    "            y = torch.LongTensor(bs).fill_(new_value)\n",
    "        attributes[:, i:j].scatter_(1, y.unsqueeze(1), 1)\n",
    "\n",
    "    if attribute_id == 'all':\n",
    "        assert new_value is None\n",
    "        for attribute_id in range(len(params.attr)):\n",
    "            flip_attribute(attribute_id)\n",
    "    else:\n",
    "        assert type(new_value) is int\n",
    "        flip_attribute(attribute_id, new_value)\n",
    "\n",
    "    return Variable(attributes.cuda())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From logger.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "class LogFormatter():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def format(self, record):\n",
    "        elapsed_seconds = round(record.created - self.start_time)\n",
    "\n",
    "        prefix = \"%s - %s - %s\" % (\n",
    "            record.levelname,\n",
    "            time.strftime('%x %X'),\n",
    "            timedelta(seconds=elapsed_seconds)\n",
    "        )\n",
    "        message = record.getMessage()\n",
    "        message = message.replace('\\n', '\\n' + ' ' * (len(prefix) + 3))\n",
    "        return \"%s - %s\" % (prefix, message)\n",
    "\n",
    "\n",
    "def create_logger(filepath):\n",
    "    \"\"\"\n",
    "    Create a logger.\n",
    "    \"\"\"\n",
    "    # create log formatter\n",
    "    log_formatter = LogFormatter()\n",
    "\n",
    "    # create file handler and set level to debug\n",
    "    if filepath is not None:\n",
    "        file_handler = logging.FileHandler(filepath, \"a\")\n",
    "        file_handler.setLevel(logging.DEBUG)\n",
    "        file_handler.setFormatter(log_formatter)\n",
    "\n",
    "    # create console handler and set level to info\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(log_formatter)\n",
    "\n",
    "    # create logger and set level to debug\n",
    "    logger = logging.getLogger()\n",
    "    logger.handlers = []\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.propagate = False\n",
    "    if filepath is not None:\n",
    "        logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    # reset logger elapsed time\n",
    "    def reset_time():\n",
    "        log_formatter.start_time = time.time()\n",
    "    logger.reset_time = reset_time\n",
    "\n",
    "    return logger\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from logging import getLogger\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "AVAILABLE_ATTR = [\n",
    "    \"5_o_Clock_Shadow\", \"Arched_Eyebrows\", \"Attractive\", \"Bags_Under_Eyes\", \"Bald\",\n",
    "    \"Bangs\", \"Big_Lips\", \"Big_Nose\", \"Black_Hair\", \"Blond_Hair\", \"Blurry\", \"Brown_Hair\",\n",
    "    \"Bushy_Eyebrows\", \"Chubby\", \"Double_Chin\", \"Eyeglasses\", \"Goatee\", \"Gray_Hair\",\n",
    "    \"Heavy_Makeup\", \"High_Cheekbones\", \"Male\", \"Mouth_Slightly_Open\", \"Mustache\",\n",
    "    \"Narrow_Eyes\", \"No_Beard\", \"Oval_Face\", \"Pale_Skin\", \"Pointy_Nose\",\n",
    "    \"Receding_Hairline\", \"Rosy_Cheeks\", \"Sideburns\", \"Smiling\", \"Straight_Hair\",\n",
    "    \"Wavy_Hair\", \"Wearing_Earrings\", \"Wearing_Hat\", \"Wearing_Lipstick\",\n",
    "    \"Wearing_Necklace\", \"Wearing_Necktie\", \"Young\"\n",
    "]\n",
    "\n",
    "# DATA_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data')\n",
    "DATA_PATH=r'C:\\GitHub\\Smart-Education-data\\data'\n",
    "\n",
    "def log_attributes_stats(train_attributes, valid_attributes, test_attributes, params):\n",
    "    \"\"\"\n",
    "    Log attributes distributions.\n",
    "    \"\"\"\n",
    "    k = 0\n",
    "    for (attr_name, n_cat) in params.attr:\n",
    "        logger.debug('Train %s: %s' % (attr_name, ' / '.join(['%.5f' % train_attributes[:, k + i].mean() for i in range(n_cat)])))\n",
    "        logger.debug('Valid %s: %s' % (attr_name, ' / '.join(['%.5f' % valid_attributes[:, k + i].mean() for i in range(n_cat)])))\n",
    "        logger.debug('Test  %s: %s' % (attr_name, ' / '.join(['%.5f' % test_attributes[:, k + i].mean() for i in range(n_cat)])))\n",
    "        assert train_attributes[:, k:k + n_cat].sum() == train_attributes.size(0)\n",
    "        assert valid_attributes[:, k:k + n_cat].sum() == valid_attributes.size(0)\n",
    "        assert test_attributes[:, k:k + n_cat].sum() == test_attributes.size(0)\n",
    "        k += n_cat\n",
    "    assert k == params.n_attr\n",
    "\n",
    "\n",
    "def load_images(params):\n",
    "    \"\"\"\n",
    "    Load celebA dataset.\n",
    "    \"\"\"\n",
    "    # load data\n",
    "    images_filename = 'images_%i_%i_20000.pth' if params.debug else 'images_%i_%i.pth'\n",
    "    images_filename = images_filename % (params.img_sz, params.img_sz)\n",
    "    # images = torch.load(os.path.join(DATA_PATH, images_filename))\n",
    "    images = torch.load(os.path.join(DATA_PATH, 'celeba-out', images_filename))\n",
    "    attributes = torch.load(os.path.join(DATA_PATH, 'attributes.pth'))\n",
    "\n",
    "    # parse attributes\n",
    "    attrs = []\n",
    "    for name, n_cat in params.attr:\n",
    "        for i in range(n_cat):\n",
    "            attrs.append(torch.FloatTensor((attributes[name] == i).astype(np.float32)))\n",
    "    attributes = torch.cat([x.unsqueeze(1) for x in attrs], 1)\n",
    "    # split train / valid / test\n",
    "    if params.debug:\n",
    "        train_index = 0\n",
    "        valid_index = 0\n",
    "        test_index = len(images)\n",
    "        # train_index = 10000\n",
    "        # valid_index = 15000\n",
    "        # test_index = 20000\n",
    "    else:\n",
    "        # train_index = 162770\n",
    "        train_index = 0\n",
    "        valid_index = 0\n",
    "        # valid_index = 162770 + 19867\n",
    "        test_index = len(images)\n",
    "    train_images = images[:train_index]\n",
    "    valid_images = images[train_index:valid_index]\n",
    "    test_images = images[valid_index:test_index]\n",
    "    train_attributes = attributes[:train_index]\n",
    "    valid_attributes = attributes[train_index:valid_index]\n",
    "    test_attributes = attributes[valid_index:test_index]\n",
    "    # log dataset statistics / return dataset\n",
    "    logger.info('%i / %i / %i images with attributes for train / valid / test sets'\n",
    "                % (len(train_images), len(valid_images), len(test_images)))\n",
    "    log_attributes_stats(train_attributes, valid_attributes, test_attributes, params)\n",
    "    images = train_images, valid_images, test_images\n",
    "    attributes = train_attributes, valid_attributes, test_attributes\n",
    "    return images, attributes\n",
    "\n",
    "\n",
    "def normalize_images(images):\n",
    "    \"\"\"\n",
    "    Normalize image values.\n",
    "    \"\"\"\n",
    "    return images.float().div_(255.0).mul_(2.0).add_(-1)\n",
    "\n",
    "\n",
    "class DataSampler(object):\n",
    "\n",
    "    def __init__(self, images, attributes, params):\n",
    "        \"\"\"\n",
    "        Initialize the data sampler with training data.\n",
    "        \"\"\"\n",
    "        assert images.size(0) == attributes.size(0), (images.size(), attributes.size())\n",
    "        self.images = images\n",
    "        self.attributes = attributes\n",
    "        self.batch_size = params.batch_size\n",
    "        self.v_flip = params.v_flip\n",
    "        self.h_flip = params.h_flip\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of images in the object dataset.\n",
    "        \"\"\"\n",
    "        return self.images.size(0)\n",
    "\n",
    "    def train_batch(self, bs):\n",
    "        \"\"\"\n",
    "        Get a batch of random images with their attributes.\n",
    "        \"\"\"\n",
    "        # image IDs\n",
    "        idx = torch.LongTensor(bs).random_(len(self.images))\n",
    "\n",
    "        # select images / attributes\n",
    "        batch_x = normalize_images(self.images.index_select(0, idx).cuda())\n",
    "        batch_y = self.attributes.index_select(0, idx).cuda()\n",
    "\n",
    "        # data augmentation\n",
    "        if self.v_flip and np.random.rand() <= 0.5:\n",
    "            batch_x = batch_x.index_select(2, torch.arange(batch_x.size(2) - 1, -1, -1).long().cuda())\n",
    "        if self.h_flip and np.random.rand() <= 0.5:\n",
    "            batch_x = batch_x.index_select(3, torch.arange(batch_x.size(3) - 1, -1, -1).long().cuda())\n",
    "\n",
    "        return Variable(batch_x, volatile=False), Variable(batch_y, volatile=False)\n",
    "\n",
    "    def eval_batch(self, i, j):\n",
    "        \"\"\"\n",
    "        Get a batch of images in a range with their attributes.\n",
    "        \"\"\"\n",
    "        assert i < j\n",
    "        batch_x = normalize_images(self.images[i:j].cuda())\n",
    "        batch_y = self.attributes[i:j].cuda()\n",
    "        return Variable(batch_x, volatile=True), Variable(batch_y, volatile=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import inspect\n",
    "import argparse\n",
    "import subprocess\n",
    "import torch\n",
    "from torch import optim\n",
    "from logging import getLogger\n",
    "\n",
    "# from .logger import create_logger # above\n",
    "# from .loader import AVAILABLE_ATTR # above\n",
    "\n",
    "\n",
    "FALSY_STRINGS = {'off', 'false', '0'}\n",
    "TRUTHY_STRINGS = {'on', 'true', '1'}\n",
    "\n",
    "# MODELS_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'models')\n",
    "MODELS_PATH=r'C:\\GitHub\\Smart-Education-data\\saved_models_fader'\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "def initialize_exp(params):\n",
    "    \"\"\"\n",
    "    Experiment initialization.\n",
    "    \"\"\"\n",
    "    # dump parameters\n",
    "    params.dump_path = get_dump_path(params)\n",
    "    pickle.dump(params, open(os.path.join(params.dump_path, 'params.pkl'), 'wb'))\n",
    "\n",
    "    # create a logger\n",
    "    logger = create_logger(os.path.join(params.dump_path, 'train.log'))\n",
    "    logger.info('============ Initialized logger ============')\n",
    "    logger.info('\\n'.join('%s: %s' % (k, str(v)) for k, v\n",
    "                          in sorted(dict(vars(params)).items())))\n",
    "    return logger\n",
    "\n",
    "\n",
    "def bool_flag(s):\n",
    "    \"\"\"\n",
    "    Parse boolean arguments from the command line.\n",
    "    \"\"\"\n",
    "    if s.lower() in FALSY_STRINGS:\n",
    "        return False\n",
    "    elif s.lower() in TRUTHY_STRINGS:\n",
    "        return True\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError(\"invalid value for a boolean flag. use 0 or 1\")\n",
    "\n",
    "\n",
    "def attr_flag(s):\n",
    "    \"\"\"\n",
    "    Parse attributes parameters.\n",
    "    \"\"\"\n",
    "    if s == \"*\":\n",
    "        return s\n",
    "    attr = s.split(',')\n",
    "    assert len(attr) == len(set(attr))\n",
    "    attributes = []\n",
    "    for x in attr:\n",
    "        if '.' not in x:\n",
    "            attributes.append((x, 2))\n",
    "        else:\n",
    "            split = x.split('.')\n",
    "            assert len(split) == 2 and len(split[0]) > 0\n",
    "            assert split[1].isdigit() and int(split[1]) >= 2\n",
    "            attributes.append((split[0], int(split[1])))\n",
    "    return sorted(attributes, key=lambda x: (x[1], x[0]))\n",
    "\n",
    "\n",
    "def check_attr(params):\n",
    "    \"\"\"\n",
    "    Check attributes validy.\n",
    "    \"\"\"\n",
    "    if params.attr == '*':\n",
    "        params.attr = attr_flag(','.join(AVAILABLE_ATTR))\n",
    "    else:\n",
    "        assert all(name in AVAILABLE_ATTR and n_cat >= 2 for name, n_cat in params.attr)\n",
    "    params.n_attr = sum([n_cat for _, n_cat in params.attr])\n",
    "\n",
    "\n",
    "def get_optimizer(model, s):\n",
    "    \"\"\"\n",
    "    Parse optimizer parameters.\n",
    "    Input should be of the form:\n",
    "        - \"sgd,lr=0.01\"\n",
    "        - \"adagrad,lr=0.1,lr_decay=0.05\"\n",
    "    \"\"\"\n",
    "    if \",\" in s:\n",
    "        method = s[:s.find(',')]\n",
    "        optim_params = {}\n",
    "        for x in s[s.find(',') + 1:].split(','):\n",
    "            split = x.split('=')\n",
    "            assert len(split) == 2\n",
    "            assert re.match(\"^[+-]?(\\d+(\\.\\d*)?|\\.\\d+)$\", split[1]) is not None\n",
    "            optim_params[split[0]] = float(split[1])\n",
    "    else:\n",
    "        method = s\n",
    "        optim_params = {}\n",
    "\n",
    "    if method == 'adadelta':\n",
    "        optim_fn = optim.Adadelta\n",
    "    elif method == 'adagrad':\n",
    "        optim_fn = optim.Adagrad\n",
    "    elif method == 'adam':\n",
    "        optim_fn = optim.Adam\n",
    "        optim_params['betas'] = (optim_params.get('beta1', 0.5), optim_params.get('beta2', 0.999))\n",
    "        optim_params.pop('beta1', None)\n",
    "        optim_params.pop('beta2', None)\n",
    "    elif method == 'adamax':\n",
    "        optim_fn = optim.Adamax\n",
    "    elif method == 'asgd':\n",
    "        optim_fn = optim.ASGD\n",
    "    elif method == 'rmsprop':\n",
    "        optim_fn = optim.RMSprop\n",
    "    elif method == 'rprop':\n",
    "        optim_fn = optim.Rprop\n",
    "    elif method == 'sgd':\n",
    "        optim_fn = optim.SGD\n",
    "        assert 'lr' in optim_params\n",
    "    else:\n",
    "        raise Exception('Unknown optimization method: \"%s\"' % method)\n",
    "\n",
    "    # check that we give good parameters to the optimizer\n",
    "    expected_args = inspect.getargspec(optim_fn.__init__)[0]\n",
    "    assert expected_args[:2] == ['self', 'params']\n",
    "    if not all(k in expected_args[2:] for k in optim_params.keys()):\n",
    "        raise Exception('Unexpected parameters: expected \"%s\", got \"%s\"' % (\n",
    "            str(expected_args[2:]), str(optim_params.keys())))\n",
    "\n",
    "    return optim_fn(model.parameters(), **optim_params)\n",
    "\n",
    "\n",
    "def clip_grad_norm(parameters, max_norm, norm_type=2):\n",
    "    \"\"\"Clips gradient norm of an iterable of parameters.\n",
    "    The norm is computed over all gradients together, as if they were\n",
    "    concatenated into a single vector. Gradients are modified in-place.\n",
    "    Arguments:\n",
    "        parameters (Iterable[Variable]): an iterable of Variables that will have\n",
    "            gradients normalized\n",
    "        max_norm (float or int): max norm of the gradients\n",
    "        norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for infinity norm.\n",
    "    \"\"\"\n",
    "    parameters = list(parameters)\n",
    "    max_norm = float(max_norm)\n",
    "    norm_type = float(norm_type)\n",
    "    if norm_type == float('inf'):\n",
    "        total_norm = max(p.grad.data.abs().max() for p in parameters)\n",
    "    else:\n",
    "        total_norm = 0\n",
    "        for p in parameters:\n",
    "            param_norm = p.grad.data.norm(norm_type)\n",
    "            total_norm += param_norm ** norm_type\n",
    "        total_norm = total_norm ** (1. / norm_type)\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    if clip_coef >= 1:\n",
    "        return\n",
    "    for p in parameters:\n",
    "        p.grad.data.mul_(clip_coef)\n",
    "\n",
    "\n",
    "def get_dump_path(params):\n",
    "    \"\"\"\n",
    "    Create a directory to store the experiment.\n",
    "    \"\"\"\n",
    "    assert os.path.isdir(MODELS_PATH)\n",
    "\n",
    "    # create the sweep path if it does not exist\n",
    "    sweep_path = os.path.join(MODELS_PATH, params.name)\n",
    "    if not os.path.exists(sweep_path):\n",
    "        subprocess.Popen(\"mkdir %s\" % sweep_path, shell=True).wait()\n",
    "\n",
    "    # create a random name for the experiment\n",
    "    chars = 'abcdefghijklmnopqrstuvwxyz0123456789'\n",
    "    while True:\n",
    "        exp_id = ''.join(random.choice(chars) for _ in range(10))\n",
    "        dump_path = os.path.join(MODELS_PATH, params.name, exp_id)\n",
    "        if not os.path.isdir(dump_path):\n",
    "            break\n",
    "\n",
    "    # create the dump folder\n",
    "    if not os.path.isdir(dump_path):\n",
    "        subprocess.Popen(\"mkdir %s\" % dump_path, shell=True).wait()\n",
    "    return dump_path\n",
    "\n",
    "\n",
    "def reload_model(model, to_reload, attributes=None):\n",
    "    \"\"\"\n",
    "    Reload a previously trained model.\n",
    "    \"\"\"\n",
    "    # reload the model\n",
    "    assert os.path.isfile(to_reload)\n",
    "    to_reload = torch.load(to_reload)\n",
    "\n",
    "    # check parameters sizes\n",
    "    model_params = set(model.state_dict().keys())\n",
    "    to_reload_params = set(to_reload.state_dict().keys())\n",
    "    assert model_params == to_reload_params, (model_params - to_reload_params,\n",
    "                                              to_reload_params - model_params)\n",
    "\n",
    "    # check attributes\n",
    "    attributes = [] if attributes is None else attributes\n",
    "    for k in attributes:\n",
    "        if getattr(model, k, None) is None:\n",
    "            raise Exception('Attribute \"%s\" not found in the current model' % k)\n",
    "        if getattr(to_reload, k, None) is None:\n",
    "            raise Exception('Attribute \"%s\" not found in the model to reload' % k)\n",
    "        if getattr(model, k) != getattr(to_reload, k):\n",
    "            raise Exception('Attribute \"%s\" differs between the current model (%s) '\n",
    "                            'and the one to reload (%s)'\n",
    "                            % (k, str(getattr(model, k)), str(getattr(to_reload, k))))\n",
    "\n",
    "    # copy saved parameters\n",
    "    for k in model.state_dict().keys():\n",
    "        if model.state_dict()[k].size() != to_reload.state_dict()[k].size():\n",
    "            raise Exception(\"Expected tensor {} of size {}, but got {}\".format(\n",
    "                k, model.state_dict()[k].size(),\n",
    "                to_reload.state_dict()[k].size()\n",
    "            ))\n",
    "        model.state_dict()[k].copy_(to_reload.state_dict()[k])\n",
    "\n",
    "\n",
    "def print_accuracies(values):\n",
    "    \"\"\"\n",
    "    Pretty plot of accuracies.\n",
    "    \"\"\"\n",
    "    assert all(len(x) == 2 for x in values)\n",
    "    for name, value in values:\n",
    "        logger.info('{:<20}: {:>6}'.format(name, '%.3f%%' % (100 * value)))\n",
    "    logger.info('')\n",
    "\n",
    "\n",
    "def get_lambda(l, params):\n",
    "    \"\"\"\n",
    "    Compute discriminators' lambdas.\n",
    "    \"\"\"\n",
    "    s = params.lambda_schedule\n",
    "    if s == 0:\n",
    "        return l\n",
    "    else:\n",
    "        return l * float(min(params.n_total_iter, s)) / s\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From evaluation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from logging import getLogger\n",
    "\n",
    "# from .model import update_predictions, flip_attributes\n",
    "# from .utils import print_accuracies\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "class Evaluator(object):\n",
    "\n",
    "    def __init__(self, ae, lat_dis, ptc_dis, clf_dis, eval_clf, data, params):\n",
    "        \"\"\"\n",
    "        Evaluator initialization.\n",
    "        \"\"\"\n",
    "        # data / parameters\n",
    "        self.data = data\n",
    "        self.params = params\n",
    "\n",
    "        # modules\n",
    "        self.ae = ae\n",
    "        self.lat_dis = lat_dis\n",
    "        self.ptc_dis = ptc_dis\n",
    "        self.clf_dis = clf_dis\n",
    "        self.eval_clf = eval_clf\n",
    "        assert eval_clf.img_sz == params.img_sz\n",
    "        assert all(attr in eval_clf.attr for attr in params.attr)\n",
    "\n",
    "    def eval_reconstruction_loss(self):\n",
    "        \"\"\"\n",
    "        Compute the autoencoder reconstruction perplexity.\n",
    "        \"\"\"\n",
    "        data = self.data\n",
    "        params = self.params\n",
    "        self.ae.eval()\n",
    "        bs = params.batch_size\n",
    "\n",
    "        costs = []\n",
    "        for i in range(0, len(data), bs):\n",
    "            batch_x, batch_y = data.eval_batch(i, i + bs)\n",
    "            _, dec_outputs = self.ae(batch_x, batch_y)\n",
    "            costs.append(((dec_outputs[-1] - batch_x) ** 2).mean().data[0])\n",
    "\n",
    "        return np.mean(costs)\n",
    "\n",
    "    def eval_lat_dis_accuracy(self):\n",
    "        \"\"\"\n",
    "        Compute the latent discriminator prediction accuracy.\n",
    "        \"\"\"\n",
    "        data = self.data\n",
    "        params = self.params\n",
    "        self.ae.eval()\n",
    "        self.lat_dis.eval()\n",
    "        bs = params.batch_size\n",
    "\n",
    "        all_preds = [[] for _ in range(len(params.attr))]\n",
    "        for i in range(0, len(data), bs):\n",
    "            batch_x, batch_y = data.eval_batch(i, i + bs)\n",
    "            enc_outputs = self.ae.encode(batch_x)\n",
    "            preds = self.lat_dis(enc_outputs[-1 - params.n_skip]).data.cpu()\n",
    "            update_predictions(all_preds, preds, batch_y.data.cpu(), params)\n",
    "\n",
    "        return [np.mean(x) for x in all_preds]\n",
    "\n",
    "    def eval_ptc_dis_accuracy(self):\n",
    "        \"\"\"\n",
    "        Compute the patch discriminator prediction accuracy.\n",
    "        \"\"\"\n",
    "        data = self.data\n",
    "        params = self.params\n",
    "        self.ae.eval()\n",
    "        self.ptc_dis.eval()\n",
    "        bs = params.batch_size\n",
    "\n",
    "        real_preds = []\n",
    "        fake_preds = []\n",
    "\n",
    "        for i in range(0, len(data), bs):\n",
    "            # batch / encode / decode\n",
    "            batch_x, batch_y = data.eval_batch(i, i + bs)\n",
    "            flipped = flip_attributes(batch_y, params, 'all')\n",
    "            _, dec_outputs = self.ae(batch_x, flipped)\n",
    "            # predictions\n",
    "            real_preds.extend(self.ptc_dis(batch_x).data.tolist())\n",
    "            fake_preds.extend(self.ptc_dis(dec_outputs[-1]).data.tolist())\n",
    "\n",
    "        return real_preds, fake_preds\n",
    "\n",
    "    def eval_clf_dis_accuracy(self):\n",
    "        \"\"\"\n",
    "        Compute the classifier discriminator prediction accuracy.\n",
    "        \"\"\"\n",
    "        data = self.data\n",
    "        params = self.params\n",
    "        self.ae.eval()\n",
    "        self.clf_dis.eval()\n",
    "        bs = params.batch_size\n",
    "\n",
    "        all_preds = [[] for _ in range(params.n_attr)]\n",
    "        for i in range(0, len(data), bs):\n",
    "            # batch / encode / decode\n",
    "            batch_x, batch_y = data.eval_batch(i, i + bs)\n",
    "            enc_outputs = self.ae.encode(batch_x)\n",
    "            # flip all attributes one by one\n",
    "            k = 0\n",
    "            for j, (_, n_cat) in enumerate(params.attr):\n",
    "                for value in range(n_cat):\n",
    "                    flipped = flip_attributes(batch_y, params, j, new_value=value)\n",
    "                    dec_outputs = self.ae.decode(enc_outputs, flipped)\n",
    "                    # classify\n",
    "                    clf_dis_preds = self.clf_dis(dec_outputs[-1])[:, j:j + n_cat].max(1)[1].view(-1)\n",
    "                    all_preds[k].extend((clf_dis_preds.data.cpu() == value).tolist())\n",
    "                    k += 1\n",
    "            assert k == params.n_attr\n",
    "\n",
    "        return [np.mean(x) for x in all_preds]\n",
    "\n",
    "    def eval_clf_accuracy(self):\n",
    "        \"\"\"\n",
    "        Compute the accuracy of flipped attributes according to the trained classifier.\n",
    "        \"\"\"\n",
    "        data = self.data\n",
    "        params = self.params\n",
    "        self.ae.eval()\n",
    "        bs = params.batch_size\n",
    "\n",
    "        idx = []\n",
    "        for j in range(len(params.attr)):\n",
    "            attr_index = self.eval_clf.attr.index(params.attr[j])\n",
    "            idx.append(sum([x[1] for x in self.eval_clf.attr[:attr_index]]))\n",
    "\n",
    "        all_preds = [[] for _ in range(params.n_attr)]\n",
    "        for i in range(0, len(data), bs):\n",
    "            # batch / encode / decode\n",
    "            batch_x, batch_y = data.eval_batch(i, i + bs)\n",
    "            enc_outputs = self.ae.encode(batch_x)\n",
    "            # flip all attributes one by one\n",
    "            k = 0\n",
    "            for j, (_, n_cat) in enumerate(params.attr):\n",
    "                for value in range(n_cat):\n",
    "                    flipped = flip_attributes(batch_y, params, j, new_value=value)\n",
    "                    dec_outputs = self.ae.decode(enc_outputs, flipped)\n",
    "                    # classify\n",
    "                    clf_preds = self.eval_clf(dec_outputs[-1])[:, idx[j]:idx[j] + n_cat].max(1)[1].view(-1)\n",
    "                    all_preds[k].extend((clf_preds.data.cpu() == value).tolist())\n",
    "                    k += 1\n",
    "            assert k == params.n_attr\n",
    "\n",
    "        return [np.mean(x) for x in all_preds]\n",
    "\n",
    "    def evaluate(self, n_epoch):\n",
    "        \"\"\"\n",
    "        Evaluate all models / log evaluation results.\n",
    "        \"\"\"\n",
    "        params = self.params\n",
    "        logger.info('')\n",
    "\n",
    "        # reconstruction loss\n",
    "        ae_loss = self.eval_reconstruction_loss()\n",
    "\n",
    "        # latent discriminator accuracy\n",
    "        log_lat_dis = []\n",
    "        if params.n_lat_dis:\n",
    "            lat_dis_accu = self.eval_lat_dis_accuracy()\n",
    "            log_lat_dis.append(('lat_dis_accu', np.mean(lat_dis_accu)))\n",
    "            for accu, (name, _) in zip(lat_dis_accu, params.attr):\n",
    "                log_lat_dis.append(('lat_dis_accu_%s' % name, accu))\n",
    "            logger.info('Latent discriminator accuracy:')\n",
    "            print_accuracies(log_lat_dis)\n",
    "\n",
    "        # patch discriminator accuracy\n",
    "        log_ptc_dis = []\n",
    "        if params.n_ptc_dis:\n",
    "            ptc_dis_real_preds, ptc_dis_fake_preds = self.eval_ptc_dis_accuracy()\n",
    "            accu_real = (np.array(ptc_dis_real_preds).astype(np.float32) >= 0.5).mean()\n",
    "            accu_fake = (np.array(ptc_dis_fake_preds).astype(np.float32) <= 0.5).mean()\n",
    "            log_ptc_dis.append(('ptc_dis_preds_real', np.mean(ptc_dis_real_preds)))\n",
    "            log_ptc_dis.append(('ptc_dis_preds_fake', np.mean(ptc_dis_fake_preds)))\n",
    "            log_ptc_dis.append(('ptc_dis_accu_real', accu_real))\n",
    "            log_ptc_dis.append(('ptc_dis_accu_fake', accu_fake))\n",
    "            log_ptc_dis.append(('ptc_dis_accu', (accu_real + accu_fake) / 2))\n",
    "            logger.info('Patch discriminator accuracy:')\n",
    "            print_accuracies(log_ptc_dis)\n",
    "\n",
    "        # classifier discriminator accuracy\n",
    "        log_clf_dis = []\n",
    "        if params.n_clf_dis:\n",
    "            clf_dis_accu = self.eval_clf_dis_accuracy()\n",
    "            k = 0\n",
    "            log_clf_dis += [('clf_dis_accu', np.mean(clf_dis_accu))]\n",
    "            for name, n_cat in params.attr:\n",
    "                log_clf_dis.append(('clf_dis_accu_%s' % name, np.mean(clf_dis_accu[k:k + n_cat])))\n",
    "                log_clf_dis.extend([('clf_dis_accu_%s_%i' % (name, j), clf_dis_accu[k + j])\n",
    "                                    for j in range(n_cat)])\n",
    "                k += n_cat\n",
    "            logger.info('Classifier discriminator accuracy:')\n",
    "            print_accuracies(log_clf_dis)\n",
    "\n",
    "        # classifier accuracy\n",
    "        log_clf = []\n",
    "        clf_accu = self.eval_clf_accuracy()\n",
    "        k = 0\n",
    "        log_clf += [('clf_accu', np.mean(clf_accu))]\n",
    "        for name, n_cat in params.attr:\n",
    "            log_clf.append(('clf_accu_%s' % name, np.mean(clf_accu[k:k + n_cat])))\n",
    "            log_clf.extend([('clf_accu_%s_%i' % (name, j), clf_accu[k + j])\n",
    "                            for j in range(n_cat)])\n",
    "            k += n_cat\n",
    "        logger.info('Classifier accuracy:')\n",
    "        print_accuracies(log_clf)\n",
    "\n",
    "        # log autoencoder loss\n",
    "        logger.info('Autoencoder loss: %.5f' % ae_loss)\n",
    "\n",
    "        # JSON log\n",
    "        to_log = dict([\n",
    "            ('n_epoch', n_epoch),\n",
    "            ('ae_loss', ae_loss)\n",
    "        ] + log_lat_dis + log_ptc_dis + log_clf_dis + log_clf)\n",
    "        logger.debug(\"__log__:%s\" % json.dumps(to_log))\n",
    "\n",
    "        return to_log\n",
    "\n",
    "\n",
    "def compute_accuracy(classifier, data, params):\n",
    "    \"\"\"\n",
    "    Compute the classifier prediction accuracy.\n",
    "    \"\"\"\n",
    "    classifier.eval()\n",
    "    bs = params.batch_size\n",
    "\n",
    "    all_preds = [[] for _ in range(len(classifier.attr))]\n",
    "    for i in range(0, len(data), bs):\n",
    "        batch_x, batch_y = data.eval_batch(i, i + bs)\n",
    "        preds = classifier(batch_x).data.cpu()\n",
    "        update_predictions(all_preds, preds, batch_y.data.cpu(), params)\n",
    "\n",
    "    return [np.mean(x) for x in all_preds]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from logging import getLogger\n",
    "\n",
    "# from .utils import get_optimizer, clip_grad_norm, get_lambda, reload_model\n",
    "# from .model import get_attr_loss, flip_attributes\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "\n",
    "    def __init__(self, ae, lat_dis, ptc_dis, clf_dis, data, params):\n",
    "        \"\"\"\n",
    "        Trainer initialization.\n",
    "        \"\"\"\n",
    "        # data / parameters\n",
    "        self.data = data\n",
    "        self.params = params\n",
    "\n",
    "        # modules\n",
    "        self.ae = ae\n",
    "        self.lat_dis = lat_dis\n",
    "        self.ptc_dis = ptc_dis\n",
    "        self.clf_dis = clf_dis\n",
    "\n",
    "        # optimizers\n",
    "        self.ae_optimizer = get_optimizer(ae, params.ae_optimizer)\n",
    "        logger.info(ae)\n",
    "        logger.info('%i parameters in the autoencoder. '\n",
    "                    % sum([p.nelement() for p in ae.parameters()]))\n",
    "        if params.n_lat_dis:\n",
    "            logger.info(lat_dis)\n",
    "            logger.info('%i parameters in the latent discriminator. '\n",
    "                        % sum([p.nelement() for p in lat_dis.parameters()]))\n",
    "            self.lat_dis_optimizer = get_optimizer(lat_dis, params.dis_optimizer)\n",
    "        if params.n_ptc_dis:\n",
    "            logger.info(ptc_dis)\n",
    "            logger.info('%i parameters in the patch discriminator. '\n",
    "                        % sum([p.nelement() for p in ptc_dis.parameters()]))\n",
    "            self.ptc_dis_optimizer = get_optimizer(ptc_dis, params.dis_optimizer)\n",
    "        if params.n_clf_dis:\n",
    "            logger.info(clf_dis)\n",
    "            logger.info('%i parameters in the classifier discriminator. '\n",
    "                        % sum([p.nelement() for p in clf_dis.parameters()]))\n",
    "            self.clf_dis_optimizer = get_optimizer(clf_dis, params.dis_optimizer)\n",
    "\n",
    "        # reload pretrained models\n",
    "        if params.ae_reload:\n",
    "            reload_model(ae, params.ae_reload,\n",
    "                         ['img_sz', 'img_fm', 'init_fm', 'n_layers', 'n_skip', 'attr', 'n_attr'])\n",
    "        if params.lat_dis_reload:\n",
    "            reload_model(lat_dis, params.lat_dis_reload,\n",
    "                         ['enc_dim', 'attr', 'n_attr'])\n",
    "        if params.ptc_dis_reload:\n",
    "            reload_model(ptc_dis, params.ptc_dis_reload,\n",
    "                         ['img_sz', 'img_fm', 'init_fm', 'max_fm', 'n_patch_dis_layers'])\n",
    "        if params.clf_dis_reload:\n",
    "            reload_model(clf_dis, params.clf_dis_reload,\n",
    "                         ['img_sz', 'img_fm', 'init_fm', 'max_fm', 'hid_dim', 'attr', 'n_attr'])\n",
    "\n",
    "        # training statistics\n",
    "        self.stats = {}\n",
    "        self.stats['rec_costs'] = []\n",
    "        self.stats['lat_dis_costs'] = []\n",
    "        self.stats['ptc_dis_costs'] = []\n",
    "        self.stats['clf_dis_costs'] = []\n",
    "\n",
    "        # best reconstruction loss / best accuracy\n",
    "        self.best_loss = 1e12\n",
    "        self.best_accu = -1e12\n",
    "        self.params.n_total_iter = 0\n",
    "\n",
    "    def lat_dis_step(self):\n",
    "        \"\"\"\n",
    "        Train the latent discriminator.\n",
    "        \"\"\"\n",
    "        data = self.data\n",
    "        params = self.params\n",
    "        self.ae.eval()\n",
    "        self.lat_dis.train()\n",
    "        bs = params.batch_size\n",
    "        # batch / encode / discriminate\n",
    "        batch_x, batch_y = data.train_batch(bs)\n",
    "        enc_outputs = self.ae.encode(Variable(batch_x.data, volatile=True))\n",
    "        preds = self.lat_dis(Variable(enc_outputs[-1 - params.n_skip].data))\n",
    "        # loss / optimize\n",
    "        loss = get_attr_loss(preds, batch_y, False, params)\n",
    "        self.stats['lat_dis_costs'].append(loss.data[0])\n",
    "        self.lat_dis_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if params.clip_grad_norm:\n",
    "            clip_grad_norm(self.lat_dis.parameters(), params.clip_grad_norm)\n",
    "        self.lat_dis_optimizer.step()\n",
    "\n",
    "    def ptc_dis_step(self):\n",
    "        \"\"\"\n",
    "        Train the patch discriminator.\n",
    "        \"\"\"\n",
    "        data = self.data\n",
    "        params = self.params\n",
    "        self.ae.eval()\n",
    "        self.ptc_dis.train()\n",
    "        bs = params.batch_size\n",
    "        # batch / encode / discriminate\n",
    "        batch_x, batch_y = data.train_batch(bs)\n",
    "        flipped = flip_attributes(batch_y, params, 'all')\n",
    "        _, dec_outputs = self.ae(Variable(batch_x.data, volatile=True), flipped)\n",
    "        real_preds = self.ptc_dis(batch_x)\n",
    "        fake_preds = self.ptc_dis(Variable(dec_outputs[-1].data))\n",
    "        y_fake = Variable(torch.FloatTensor(real_preds.size())\n",
    "                               .fill_(params.smooth_label).cuda())\n",
    "        # loss / optimize\n",
    "        loss = F.binary_cross_entropy(real_preds, 1 - y_fake)\n",
    "        loss += F.binary_cross_entropy(fake_preds, y_fake)\n",
    "        self.stats['ptc_dis_costs'].append(loss.data[0])\n",
    "        self.ptc_dis_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if params.clip_grad_norm:\n",
    "            clip_grad_norm(self.ptc_dis.parameters(), params.clip_grad_norm)\n",
    "        self.ptc_dis_optimizer.step()\n",
    "\n",
    "    def clf_dis_step(self):\n",
    "        \"\"\"\n",
    "        Train the classifier discriminator.\n",
    "        \"\"\"\n",
    "        data = self.data\n",
    "        params = self.params\n",
    "        self.clf_dis.train()\n",
    "        bs = params.batch_size\n",
    "        # batch / predict\n",
    "        batch_x, batch_y = data.train_batch(bs)\n",
    "        preds = self.clf_dis(batch_x)\n",
    "        # loss / optimize\n",
    "        loss = get_attr_loss(preds, batch_y, False, params)\n",
    "        self.stats['clf_dis_costs'].append(loss.data[0])\n",
    "        self.clf_dis_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if params.clip_grad_norm:\n",
    "            clip_grad_norm(self.clf_dis.parameters(), params.clip_grad_norm)\n",
    "        self.clf_dis_optimizer.step()\n",
    "\n",
    "    def autoencoder_step(self):\n",
    "        \"\"\"\n",
    "        Train the autoencoder with cross-entropy loss.\n",
    "        Train the encoder with discriminator loss.\n",
    "        \"\"\"\n",
    "        data = self.data\n",
    "        params = self.params\n",
    "        self.ae.train()\n",
    "        if params.n_lat_dis:\n",
    "            self.lat_dis.eval()\n",
    "        if params.n_ptc_dis:\n",
    "            self.ptc_dis.eval()\n",
    "        if params.n_clf_dis:\n",
    "            self.clf_dis.eval()\n",
    "        bs = params.batch_size\n",
    "        # batch / encode / decode\n",
    "        batch_x, batch_y = data.train_batch(bs)\n",
    "        enc_outputs, dec_outputs = self.ae(batch_x, batch_y)\n",
    "        # autoencoder loss from reconstruction\n",
    "        loss = params.lambda_ae * ((batch_x - dec_outputs[-1]) ** 2).mean()\n",
    "        self.stats['rec_costs'].append(loss.data[0])\n",
    "        # encoder loss from the latent discriminator\n",
    "        if params.lambda_lat_dis:\n",
    "            lat_dis_preds = self.lat_dis(enc_outputs[-1 - params.n_skip])\n",
    "            lat_dis_loss = get_attr_loss(lat_dis_preds, batch_y, True, params)\n",
    "            loss = loss + get_lambda(params.lambda_lat_dis, params) * lat_dis_loss\n",
    "        # decoding with random labels\n",
    "        if params.lambda_ptc_dis + params.lambda_clf_dis > 0:\n",
    "            flipped = flip_attributes(batch_y, params, 'all')\n",
    "            dec_outputs_flipped = self.ae.decode(enc_outputs, flipped)\n",
    "        # autoencoder loss from the patch discriminator\n",
    "        if params.lambda_ptc_dis:\n",
    "            ptc_dis_preds = self.ptc_dis(dec_outputs_flipped[-1])\n",
    "            y_fake = Variable(torch.FloatTensor(ptc_dis_preds.size())\n",
    "                                   .fill_(params.smooth_label).cuda())\n",
    "            ptc_dis_loss = F.binary_cross_entropy(ptc_dis_preds, 1 - y_fake)\n",
    "            loss = loss + get_lambda(params.lambda_ptc_dis, params) * ptc_dis_loss\n",
    "        # autoencoder loss from the classifier discriminator\n",
    "        if params.lambda_clf_dis:\n",
    "            clf_dis_preds = self.clf_dis(dec_outputs_flipped[-1])\n",
    "            clf_dis_loss = get_attr_loss(clf_dis_preds, flipped, False, params)\n",
    "            loss = loss + get_lambda(params.lambda_clf_dis, params) * clf_dis_loss\n",
    "        # check NaN\n",
    "        if (loss != loss).data.any():\n",
    "            logger.error(\"NaN detected\")\n",
    "            exit()\n",
    "        # optimize\n",
    "        self.ae_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if params.clip_grad_norm:\n",
    "            clip_grad_norm(self.ae.parameters(), params.clip_grad_norm)\n",
    "        self.ae_optimizer.step()\n",
    "\n",
    "    def step(self, n_iter):\n",
    "        \"\"\"\n",
    "        End training iteration / print training statistics.\n",
    "        \"\"\"\n",
    "        # average loss\n",
    "        if len(self.stats['rec_costs']) >= 25:\n",
    "            mean_loss = [\n",
    "                ('Latent discriminator', 'lat_dis_costs'),\n",
    "                ('Patch discriminator', 'ptc_dis_costs'),\n",
    "                ('Classifier discriminator', 'clf_dis_costs'),\n",
    "                ('Reconstruction loss', 'rec_costs'),\n",
    "            ]\n",
    "            logger.info(('%06i - ' % n_iter) +\n",
    "                        ' / '.join(['%s : %.5f' % (a, np.mean(self.stats[b]))\n",
    "                                    for a, b in mean_loss if len(self.stats[b]) > 0]))\n",
    "            del self.stats['rec_costs'][:]\n",
    "            del self.stats['lat_dis_costs'][:]\n",
    "            del self.stats['ptc_dis_costs'][:]\n",
    "            del self.stats['clf_dis_costs'][:]\n",
    "\n",
    "        self.params.n_total_iter += 1\n",
    "\n",
    "    def save_model(self, name):\n",
    "        \"\"\"\n",
    "        Save the model.\n",
    "        \"\"\"\n",
    "        def save(model, filename):\n",
    "            path = os.path.join(self.params.dump_path, '%s_%s.pth' % (name, filename))\n",
    "            logger.info('Saving %s to %s ...' % (filename, path))\n",
    "            torch.save(model, path)\n",
    "        save(self.ae, 'ae')\n",
    "        if self.params.n_lat_dis:\n",
    "            save(self.lat_dis, 'lat_dis')\n",
    "        if self.params.n_ptc_dis:\n",
    "            save(self.ptc_dis, 'ptc_dis')\n",
    "        if self.params.n_clf_dis:\n",
    "            save(self.clf_dis, 'clf_dis')\n",
    "\n",
    "    def save_best_periodic(self, to_log):\n",
    "        \"\"\"\n",
    "        Save the best models / periodically save the models.\n",
    "        \"\"\"\n",
    "        if to_log['ae_loss'] < self.best_loss:\n",
    "            self.best_loss = to_log['ae_loss']\n",
    "            logger.info('Best reconstruction loss: %.5f' % self.best_loss)\n",
    "            self.save_model('best_rec')\n",
    "        if self.params.eval_clf and np.mean(to_log['clf_accu']) > self.best_accu:\n",
    "            self.best_accu = np.mean(to_log['clf_accu'])\n",
    "            logger.info('Best evaluation accuracy: %.5f' % self.best_accu)\n",
    "            self.save_model('best_accu')\n",
    "        if to_log['n_epoch'] % 5 == 0 and to_log['n_epoch'] > 0:\n",
    "            self.save_model('periodic-%i' % to_log['n_epoch'])\n",
    "\n",
    "\n",
    "def classifier_step(classifier, optimizer, data, params, costs):\n",
    "    \"\"\"\n",
    "    Train the classifier.\n",
    "    \"\"\"\n",
    "    classifier.train()\n",
    "    bs = params.batch_size\n",
    "\n",
    "    # batch / classify\n",
    "    batch_x, batch_y = data.train_batch(bs)\n",
    "    preds = classifier(batch_x)\n",
    "    # loss / optimize\n",
    "    loss = get_attr_loss(preds, batch_y, False, params)\n",
    "    costs.append(loss.data[0])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    if params.clip_grad_norm:\n",
    "        clip_grad_norm(classifier.parameters(), params.clip_grad_norm)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m params\u001b[39m.\u001b[39mreload \u001b[39mor\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(params\u001b[39m.\u001b[39mreload)\n\u001b[0;32m     64\u001b[0m \u001b[39m# initialize experiment / load dataset\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m logger \u001b[39m=\u001b[39m initialize_exp(params)\n\u001b[0;32m     66\u001b[0m data, attributes \u001b[39m=\u001b[39m load_images(params)\n\u001b[0;32m     67\u001b[0m train_data \u001b[39m=\u001b[39m DataSampler(data[\u001b[39m0\u001b[39m], attributes[\u001b[39m0\u001b[39m], params)\n",
      "Cell \u001b[1;32mIn[16], line 37\u001b[0m, in \u001b[0;36minitialize_exp\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39mExperiment initialization.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39m# dump parameters\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m params\u001b[39m.\u001b[39mdump_path \u001b[39m=\u001b[39m get_dump_path(params)\n\u001b[0;32m     38\u001b[0m pickle\u001b[39m.\u001b[39mdump(params, \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(params\u001b[39m.\u001b[39mdump_path, \u001b[39m'\u001b[39m\u001b[39mparams.pkl\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     40\u001b[0m \u001b[39m# create a logger\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 175\u001b[0m, in \u001b[0;36mget_dump_path\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_dump_path\u001b[39m(params):\n\u001b[0;32m    172\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[39m    Create a directory to store the experiment.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[39massert\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(MODELS_PATH)\n\u001b[0;32m    177\u001b[0m     \u001b[39m# create the sweep path if it does not exist\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     sweep_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(MODELS_PATH, params\u001b[39m.\u001b[39mname)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# from src.loader import load_images, DataSampler\n",
    "# from src.utils import initialize_exp, bool_flag, attr_flag, check_attr\n",
    "# from src.utils import get_optimizer, reload_model, print_accuracies\n",
    "# from src.model import Classifier\n",
    "# from src.training import classifier_step\n",
    "# from src.evaluation import compute_accuracy\n",
    "\n",
    "\n",
    "# parse parameters\n",
    "parser = argparse.ArgumentParser(description='Classifier')\n",
    "parser.add_argument(\"--name\", type=str, default=\"default\",\n",
    "                    help=\"Experiment name\")\n",
    "parser.add_argument(\"--img_sz\", type=int, default=128,#256,\n",
    "                    help=\"Image sizes (images have to be squared)\")\n",
    "parser.add_argument(\"--img_fm\", type=int, default=3,\n",
    "                    help=\"Number of feature maps (1 for grayscale, 3 for RGB)\")\n",
    "parser.add_argument(\"--attr\", type=attr_flag, default=\"Smiling\",\n",
    "                    help=\"Attributes to classify\")\n",
    "parser.add_argument(\"--init_fm\", type=int, default=32,\n",
    "                    help=\"Number of initial filters in the encoder\")\n",
    "parser.add_argument(\"--max_fm\", type=int, default=512,\n",
    "                    help=\"Number maximum of filters in the autoencoder\")\n",
    "parser.add_argument(\"--hid_dim\", type=int, default=512,\n",
    "                    help=\"Last hidden layer dimension\")\n",
    "parser.add_argument(\"--v_flip\", type=bool_flag, default=False,\n",
    "                    help=\"Random vertical flip for data augmentation\")\n",
    "parser.add_argument(\"--h_flip\", type=bool_flag, default=True,\n",
    "                    help=\"Random horizontal flip for data augmentation\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32,\n",
    "                    help=\"Batch size\")\n",
    "parser.add_argument(\"--optimizer\", type=str, default=\"adam\",\n",
    "                    help=\"Classifier optimizer (SGD / RMSprop / Adam, etc.)\")\n",
    "parser.add_argument(\"--clip_grad_norm\", type=float, default=5,\n",
    "                    help=\"Clip gradient norms (0 to disable)\")\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=1000,\n",
    "                    help=\"Total number of epochs\")\n",
    "parser.add_argument(\"--epoch_size\", type=int, default=50000,\n",
    "                    help=\"Number of samples per epoch\")\n",
    "parser.add_argument(\"--reload\", type=str, default=\"\",\n",
    "                    help=\"Reload a pretrained classifier\")\n",
    "parser.add_argument(\"--debug\", type=bool_flag, default=False,\n",
    "                    help=\"Debug mode (only load a subset of the whole dataset)\")\n",
    "# params = parser.parse_args()\n",
    "params = parser.parse_args('')\n",
    "\n",
    "# check parameters\n",
    "check_attr(params)\n",
    "assert len(params.name.strip()) > 0\n",
    "assert not params.reload or os.path.isfile(params.reload)\n",
    "\n",
    "# initialize experiment / load dataset\n",
    "logger = initialize_exp(params)\n",
    "data, attributes = load_images(params)\n",
    "train_data = DataSampler(data[0], attributes[0], params)\n",
    "valid_data = DataSampler(data[1], attributes[1], params)\n",
    "test_data = DataSampler(data[2], attributes[2], params)\n",
    "\n",
    "# build the model / reload / optimizer\n",
    "classifier = Classifier(params).cuda()\n",
    "if params.reload:\n",
    "    reload_model(classifier, params.reload,\n",
    "                 ['img_sz', 'img_fm', 'init_fm', 'hid_dim', 'attr', 'n_attr'])\n",
    "optimizer = get_optimizer(classifier, params.optimizer)\n",
    "\n",
    "\n",
    "def save_model(name):\n",
    "    \"\"\"\n",
    "    Save the model.\n",
    "    \"\"\"\n",
    "    path = os.path.join(params.dump_path, '%s.pth' % name)\n",
    "    logger.info('Saving the classifier to %s ...' % path)\n",
    "    torch.save(classifier, path)\n",
    "\n",
    "\n",
    "# best accuracy\n",
    "best_accu = -1e12\n",
    "\n",
    "\n",
    "for n_epoch in range(params.n_epochs):\n",
    "\n",
    "    logger.info('Starting epoch %i...' % n_epoch)\n",
    "    costs = []\n",
    "\n",
    "    classifier.train()\n",
    "\n",
    "    for n_iter in range(0, params.epoch_size, params.batch_size):\n",
    "\n",
    "        # classifier training\n",
    "        classifier_step(classifier, optimizer, train_data, params, costs)\n",
    "\n",
    "        # average loss\n",
    "        if len(costs) >= 25:\n",
    "            logger.info('%06i - Classifier loss: %.5f' % (n_iter, np.mean(costs)))\n",
    "            del costs[:]\n",
    "\n",
    "    # compute accuracy\n",
    "    valid_accu = compute_accuracy(classifier, valid_data, params)\n",
    "    test_accu = compute_accuracy(classifier, test_data, params)\n",
    "\n",
    "    # log classifier accuracy\n",
    "    log_accu = [('valid_accu', np.mean(valid_accu)), ('test_accu', np.mean(test_accu))]\n",
    "    for accu, (name, _) in zip(valid_accu, params.attr):\n",
    "        log_accu.append(('valid_accu_%s' % name, accu))\n",
    "    for accu, (name, _) in zip(test_accu, params.attr):\n",
    "        log_accu.append(('test_accu_%s' % name, accu))\n",
    "    logger.info('Classifier accuracy:')\n",
    "    print_accuracies(log_accu)\n",
    "\n",
    "    # JSON log\n",
    "    logger.debug(\"__log__:%s\" % json.dumps(dict([('n_epoch', n_epoch)] + log_accu)))\n",
    "\n",
    "    # save best or periodic model\n",
    "    if np.mean(valid_accu) > best_accu:\n",
    "        best_accu = np.mean(valid_accu)\n",
    "        logger.info('Best validation average accuracy: %.5f' % best_accu)\n",
    "        save_model('best')\n",
    "    elif n_epoch % 10 == 0 and n_epoch > 0:\n",
    "        save_model('periodic-%i' % n_epoch)\n",
    "\n",
    "    logger.info('End of epoch %i.\\n' % n_epoch)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 106\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m params\u001b[39m.\u001b[39mptc_dis_reload \u001b[39mor\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(params\u001b[39m.\u001b[39mptc_dis_reload)\n\u001b[0;32m    105\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m params\u001b[39m.\u001b[39mclf_dis_reload \u001b[39mor\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(params\u001b[39m.\u001b[39mclf_dis_reload)\n\u001b[1;32m--> 106\u001b[0m \u001b[39massert\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(params\u001b[39m.\u001b[39meval_clf)\n\u001b[0;32m    107\u001b[0m \u001b[39massert\u001b[39;00m params\u001b[39m.\u001b[39mlambda_lat_dis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m params\u001b[39m.\u001b[39mn_lat_dis \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    108\u001b[0m \u001b[39massert\u001b[39;00m params\u001b[39m.\u001b[39mlambda_ptc_dis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m params\u001b[39m.\u001b[39mn_ptc_dis \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "# from src.loader import load_images, DataSampler\n",
    "# from src.utils import initialize_exp, bool_flag, attr_flag, check_attr\n",
    "# from src.model import AutoEncoder, LatentDiscriminator, PatchDiscriminator, Classifier\n",
    "# from src.training import Trainer\n",
    "# from src.evaluation import Evaluator\n",
    "\n",
    "\n",
    "# parse parameters\n",
    "parser = argparse.ArgumentParser(description='Images autoencoder')\n",
    "parser.add_argument(\"--name\", type=str, default=\"default\",\n",
    "                    help=\"Experiment name\")\n",
    "parser.add_argument(\"--img_sz\", type=int, default=128,#256,\n",
    "                    help=\"Image sizes (images have to be squared)\")\n",
    "parser.add_argument(\"--img_fm\", type=int, default=3,\n",
    "                    help=\"Number of feature maps (1 for grayscale, 3 for RGB)\")\n",
    "parser.add_argument(\"--attr\", type=attr_flag, default=\"Smiling,Male\",\n",
    "                    help=\"Attributes to classify\")\n",
    "parser.add_argument(\"--instance_norm\", type=bool_flag, default=False,\n",
    "                    help=\"Use instance normalization instead of batch normalization\")\n",
    "parser.add_argument(\"--init_fm\", type=int, default=32,\n",
    "                    help=\"Number of initial filters in the encoder\")\n",
    "parser.add_argument(\"--max_fm\", type=int, default=512,\n",
    "                    help=\"Number maximum of filters in the autoencoder\")\n",
    "parser.add_argument(\"--n_layers\", type=int, default=6,\n",
    "                    help=\"Number of layers in the encoder / decoder\")\n",
    "parser.add_argument(\"--n_skip\", type=int, default=0,\n",
    "                    help=\"Number of skip connections\")\n",
    "parser.add_argument(\"--deconv_method\", type=str, default=\"convtranspose\",\n",
    "                    help=\"Deconvolution method\")\n",
    "parser.add_argument(\"--hid_dim\", type=int, default=512,\n",
    "                    help=\"Last hidden layer dimension for discriminator / classifier\")\n",
    "parser.add_argument(\"--dec_dropout\", type=float, default=0.,\n",
    "                    help=\"Dropout in the decoder\")\n",
    "parser.add_argument(\"--lat_dis_dropout\", type=float, default=0.3,\n",
    "                    help=\"Dropout in the latent discriminator\")\n",
    "parser.add_argument(\"--n_lat_dis\", type=int, default=1,\n",
    "                    help=\"Number of latent discriminator training steps\")\n",
    "parser.add_argument(\"--n_ptc_dis\", type=int, default=0,\n",
    "                    help=\"Number of patch discriminator training steps\")\n",
    "parser.add_argument(\"--n_clf_dis\", type=int, default=0,\n",
    "                    help=\"Number of classifier discriminator training steps\")\n",
    "parser.add_argument(\"--smooth_label\", type=float, default=0.2,\n",
    "                    help=\"Smooth label for patch discriminator\")\n",
    "parser.add_argument(\"--lambda_ae\", type=float, default=1,\n",
    "                    help=\"Autoencoder loss coefficient\")\n",
    "parser.add_argument(\"--lambda_lat_dis\", type=float, default=0.0001,\n",
    "                    help=\"Latent discriminator loss feedback coefficient\")\n",
    "parser.add_argument(\"--lambda_ptc_dis\", type=float, default=0,\n",
    "                    help=\"Patch discriminator loss feedback coefficient\")\n",
    "parser.add_argument(\"--lambda_clf_dis\", type=float, default=0,\n",
    "                    help=\"Classifier discriminator loss feedback coefficient\")\n",
    "parser.add_argument(\"--lambda_schedule\", type=float, default=500000,\n",
    "                    help=\"Progressively increase discriminators' lambdas (0 to disable)\")\n",
    "parser.add_argument(\"--v_flip\", type=bool_flag, default=False,\n",
    "                    help=\"Random vertical flip for data augmentation\")\n",
    "parser.add_argument(\"--h_flip\", type=bool_flag, default=True,\n",
    "                    help=\"Random horizontal flip for data augmentation\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32,\n",
    "                    help=\"Batch size\")\n",
    "parser.add_argument(\"--ae_optimizer\", type=str, default=\"adam,lr=0.0002\",\n",
    "                    help=\"Autoencoder optimizer (SGD / RMSprop / Adam, etc.)\")\n",
    "parser.add_argument(\"--dis_optimizer\", type=str, default=\"adam,lr=0.0002\",\n",
    "                    help=\"Discriminator optimizer (SGD / RMSprop / Adam, etc.)\")\n",
    "parser.add_argument(\"--clip_grad_norm\", type=float, default=5,\n",
    "                    help=\"Clip gradient norms (0 to disable)\")\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=1000,\n",
    "                    help=\"Total number of epochs\")\n",
    "parser.add_argument(\"--epoch_size\", type=int, default=50000,\n",
    "                    help=\"Number of samples per epoch\")\n",
    "parser.add_argument(\"--ae_reload\", type=str, default=\"\",\n",
    "                    help=\"Reload a pretrained encoder\")\n",
    "parser.add_argument(\"--lat_dis_reload\", type=str, default=\"\",\n",
    "                    help=\"Reload a pretrained latent discriminator\")\n",
    "parser.add_argument(\"--ptc_dis_reload\", type=str, default=\"\",\n",
    "                    help=\"Reload a pretrained patch discriminator\")\n",
    "parser.add_argument(\"--clf_dis_reload\", type=str, default=\"\",\n",
    "                    help=\"Reload a pretrained classifier discriminator\")\n",
    "parser.add_argument(\"--eval_clf\", type=str, default=\"\",\n",
    "                    help=\"Load an external classifier for evaluation\")\n",
    "parser.add_argument(\"--debug\", type=bool_flag, default=False,\n",
    "                    help=\"Debug mode (only load a subset of the whole dataset)\")\n",
    "# params = parser.parse_args()\n",
    "params = parser.parse_args('')\n",
    "\n",
    "# check parameters\n",
    "check_attr(params)\n",
    "assert len(params.name.strip()) > 0\n",
    "assert params.n_skip <= params.n_layers - 1\n",
    "assert params.deconv_method in ['convtranspose', 'upsampling', 'pixelshuffle']\n",
    "assert 0 <= params.smooth_label < 0.5\n",
    "assert not params.ae_reload or os.path.isfile(params.ae_reload)\n",
    "assert not params.lat_dis_reload or os.path.isfile(params.lat_dis_reload)\n",
    "assert not params.ptc_dis_reload or os.path.isfile(params.ptc_dis_reload)\n",
    "assert not params.clf_dis_reload or os.path.isfile(params.clf_dis_reload)\n",
    "assert os.path.isfile(params.eval_clf)\n",
    "assert params.lambda_lat_dis == 0 or params.n_lat_dis > 0\n",
    "assert params.lambda_ptc_dis == 0 or params.n_ptc_dis > 0\n",
    "assert params.lambda_clf_dis == 0 or params.n_clf_dis > 0\n",
    "\n",
    "# initialize experiment / load dataset\n",
    "logger = initialize_exp(params)\n",
    "data, attributes = load_images(params)\n",
    "train_data = DataSampler(data[0], attributes[0], params)\n",
    "valid_data = DataSampler(data[1], attributes[1], params)\n",
    "\n",
    "# build the model\n",
    "ae = AutoEncoder(params).cuda()\n",
    "lat_dis = LatentDiscriminator(params).cuda() if params.n_lat_dis else None\n",
    "ptc_dis = PatchDiscriminator(params).cuda() if params.n_ptc_dis else None\n",
    "clf_dis = Classifier(params).cuda() if params.n_clf_dis else None\n",
    "eval_clf = torch.load(params.eval_clf).cuda().eval()\n",
    "\n",
    "# trainer / evaluator\n",
    "trainer = Trainer(ae, lat_dis, ptc_dis, clf_dis, train_data, params)\n",
    "evaluator = Evaluator(ae, lat_dis, ptc_dis, clf_dis, eval_clf, valid_data, params)\n",
    "\n",
    "\n",
    "for n_epoch in range(params.n_epochs):\n",
    "\n",
    "    logger.info('Starting epoch %i...' % n_epoch)\n",
    "\n",
    "    for n_iter in range(0, params.epoch_size, params.batch_size):\n",
    "\n",
    "        # latent discriminator training\n",
    "        for _ in range(params.n_lat_dis):\n",
    "            trainer.lat_dis_step()\n",
    "\n",
    "        # patch discriminator training\n",
    "        for _ in range(params.n_ptc_dis):\n",
    "            trainer.ptc_dis_step()\n",
    "\n",
    "        # classifier discriminator training\n",
    "        for _ in range(params.n_clf_dis):\n",
    "            trainer.clf_dis_step()\n",
    "\n",
    "        # autoencoder training\n",
    "        trainer.autoencoder_step()\n",
    "\n",
    "        # print training statistics\n",
    "        trainer.step(n_iter)\n",
    "\n",
    "    # run all evaluations / save best or periodic model\n",
    "    to_log = evaluator.evaluate(n_epoch)\n",
    "    trainer.save_best_periodic(to_log)\n",
    "    logger.info('End of epoch %i.\\n' % n_epoch)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From interpolate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\GitHub\\FaderNetworks\\models\\narrow_eyes.pth\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39m# create logger / load trained model\u001b[39;00m\n\u001b[0;32m     50\u001b[0m logger \u001b[39m=\u001b[39m create_logger(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m---> 51\u001b[0m ae \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(params\u001b[39m.\u001b[39;49mmodel_path)\u001b[39m.\u001b[39meval()\n\u001b[0;32m     53\u001b[0m \u001b[39m# restore main parameters\u001b[39;00m\n\u001b[0;32m     54\u001b[0m params\u001b[39m.\u001b[39mdebug \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ranjani\\miniconda3\\lib\\site-packages\\torch\\serialization.py:795\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    794\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 795\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[1;32mc:\\Users\\Ranjani\\miniconda3\\lib\\site-packages\\torch\\serialization.py:1012\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1010\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1011\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1012\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1014\u001b[0m deserialized_storage_keys \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1016\u001b[0m offset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell() \u001b[39mif\u001b[39;00m f_should_read_directly \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ranjani\\miniconda3\\lib\\site-packages\\torch\\serialization.py:828\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.UnpicklerWrapper.find_class\u001b[1;34m(self, mod_name, name)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m--> 828\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfind_class(mod_name, name)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.image\n",
    "\n",
    "# from src.logger import create_logger\n",
    "# from src.loader import load_images, DataSampler\n",
    "# from src.utils import bool_flag\n",
    "\n",
    "\n",
    "# parse parameters\n",
    "parser = argparse.ArgumentParser(description='Attributes swapping')\n",
    "parser.add_argument(\"--model_path\", type=str, default=r\"C:\\GitHub\\FaderNetworks\\models\\narrow_eyes.pth\",\n",
    "                    help=\"Trained model path\")\n",
    "parser.add_argument(\"--n_images\", type=int, default=9,#10,\n",
    "                    help=\"Number of images to modify\")\n",
    "parser.add_argument(\"--offset\", type=int, default=0,\n",
    "                    help=\"First image index\")\n",
    "parser.add_argument(\"--n_interpolations\", type=int, default=10,\n",
    "                    help=\"Number of interpolations per image\")\n",
    "parser.add_argument(\"--alpha_min\", type=float, default=10.0,#1,\n",
    "                    help=\"Min interpolation value\")\n",
    "parser.add_argument(\"--alpha_max\", type=float, default=10.0,#1,\n",
    "                    help=\"Max interpolation value\")\n",
    "parser.add_argument(\"--plot_size\", type=int, default=5,\n",
    "                    help=\"Size of images in the grid\")\n",
    "parser.add_argument(\"--row_wise\", type=bool_flag, default=True,\n",
    "                    help=\"Represent image interpolations horizontally\")\n",
    "parser.add_argument(\"--output_path\", type=str, default='narrow_eyes.png',#\"output.png\",\n",
    "                    help=\"Output path\")\n",
    "params = parser.parse_args('')\n",
    "\n",
    "print(params.model_path)\n",
    "      \n",
    "# check parameters\n",
    "assert os.path.isfile(params.model_path)\n",
    "assert params.n_images >= 1 and params.n_interpolations >= 2\n",
    "\n",
    "# create logger / load trained model\n",
    "logger = create_logger(None)\n",
    "ae = torch.load(params.model_path).eval()\n",
    "\n",
    "# restore main parameters\n",
    "params.debug = True\n",
    "params.batch_size = 32\n",
    "params.v_flip = False\n",
    "params.h_flip = False\n",
    "params.img_sz = ae.img_sz\n",
    "params.attr = ae.attr\n",
    "params.n_attr = ae.n_attr\n",
    "if not (len(params.attr) == 1 and params.n_attr == 2):\n",
    "    raise Exception(\"The model must use a single boolean attribute only.\")\n",
    "\n",
    "# load dataset\n",
    "data, attributes = load_images(params)\n",
    "test_data = DataSampler(data[2], attributes[2], params)\n",
    "\n",
    "\n",
    "def get_interpolations(ae, images, attributes, params):\n",
    "    \"\"\"\n",
    "    Reconstruct images / create interpolations\n",
    "    \"\"\"\n",
    "    assert len(images) == len(attributes)\n",
    "    enc_outputs = ae.encode(images)\n",
    "\n",
    "    # interpolation values\n",
    "    alphas = np.linspace(1 - params.alpha_min, params.alpha_max, params.n_interpolations)\n",
    "    alphas = [torch.FloatTensor([1 - alpha, alpha]) for alpha in alphas]\n",
    "\n",
    "    # original image / reconstructed image / interpolations\n",
    "    outputs = []\n",
    "    outputs.append(images)\n",
    "    outputs.append(ae.decode(enc_outputs, attributes)[-1])\n",
    "    for alpha in alphas:\n",
    "        alpha = Variable(alpha.unsqueeze(0).expand((len(images), 2)).cuda())\n",
    "        outputs.append(ae.decode(enc_outputs, alpha)[-1])\n",
    "\n",
    "    # return stacked images\n",
    "    return torch.cat([x.unsqueeze(1) for x in outputs], 1).data.cpu()\n",
    "\n",
    "\n",
    "interpolations = []\n",
    "\n",
    "for k in range(0, params.n_images, 100):\n",
    "# for k in range(0, params.n_images, 10):\n",
    "    i = params.offset + k\n",
    "    j = params.offset + min(params.n_images, k + 100)\n",
    "    # j = params.offset + min(params.n_images, k + 10)\n",
    "    images, attributes = test_data.eval_batch(i, j)\n",
    "    #debug\n",
    "    # print('images:', images)\n",
    "    # print('attributes:', attributes)\n",
    "    # print('params', params )\n",
    "    # _=get_interpolations(ae, images, attributes, params)\n",
    "    # interpolations.append(_)\n",
    "    # print(_)\n",
    "    interpolations.append(get_interpolations(ae, images, attributes, params))\n",
    "    # print('k', k)\n",
    "# print('debug11', interpolations)\n",
    "interpolations = torch.cat(interpolations, 0)\n",
    "# print('deb1',tuple(interpolations.size()))\n",
    "# print('deb2',(params.n_images, 2 + params.n_interpolations,\n",
    "#                                  3, params.img_sz, params.img_sz))\n",
    "# print(list(interpolations.size()) == list((params.img_sz, 2 + params.n_interpolations, 3, params.img_sz, params.img_sz)))\n",
    "# assert list(interpolations.size()) == list((params.img_sz, 2 + params.n_interpolations, 3, params.img_sz, params.img_sz))\n",
    "\n",
    "def get_grid(images, row_wise, plot_size=5):\n",
    "    \"\"\"\n",
    "    Create a grid with all images.\n",
    "    \"\"\"\n",
    "    n_images, n_columns, img_fm, img_sz, _ = images.size()\n",
    "    if not row_wise:\n",
    "        images = images.transpose(0, 1).contiguous()\n",
    "    images = images.view(n_images * n_columns, img_fm, img_sz, img_sz)\n",
    "    images.add_(1).div_(2.0)\n",
    "    return make_grid(images, nrow=(n_columns if row_wise else n_images))\n",
    "\n",
    "\n",
    "# generate the grid / save it to a PNG file\n",
    "grid = get_grid(interpolations, params.row_wise, params.plot_size)\n",
    "matplotlib.image.imsave(params.output_path, grid.numpy().transpose((1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e627722d0d91bb7c18f82524675a9d23c045070d407cc9ef448429e6ca73a475"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
